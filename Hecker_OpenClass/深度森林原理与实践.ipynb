{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26597c3-acf0-4ba4-8355-dbd95a007f27",
   "metadata": {},
   "source": [
    "# <font face=\"仿宋\">课程说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71ad03-bd65-42b3-a0d5-b55e6c6bdf17",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">小伙伴好呀\\~欢迎来到《机器学习实战训练营》试学体验课！我是课程主讲老师，九天。本期公开课主题为《模型融合与深度森林》，将重点探讨时下最为流行的模型融合及其优化方法，以及机器学习最新理论成果——深度森林算法的基本原理与实践方法。         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc9eba-2343-4ee7-9bec-84347a217d47",
   "metadata": {},
   "source": [
    "- 课件领取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd996d-bc9f-4b45-b186-f8f8a1c96c4e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">公开课所有课件、代码和数据，均可在直播期间免费获取。扫描二维码添加官方客服小可爱，或者手动添加微信号littlebird_0228，并回复“ML618”即可获取哦～"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ea742-c835-4746-881d-05fa461c48fc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h2yiv3wpxjj20ox0p5dhg.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da9599-750d-4882-bdcc-99fb842b3e66",
   "metadata": {},
   "source": [
    "- 节选自《机器学习实战训练营》付费正课"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41a6f1-cfe6-4903-94b1-70f1e7a28197",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">公开课所有内容都节选自《机器学习实战训练营》付费正课。付费课程为130+小时完整体系大课，包括经典机器学习算法、集成算法、特征工程、模型融合、Kaggle竞赛案例与企业实战案例六大板块内容，想要获得更加完整、深入的学习体验，也欢迎大家报名《机器学习实战训练营》付费正课哦～"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3cf73-d4db-404a-a2a4-8233b6e57770",
   "metadata": {},
   "source": [
    "- 限时折扣"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e68ca1-c628-4be1-995b-10d0a3418398",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">《机器学习实战训练营》付费正课目前正在618年中大促中，限时半价！私聊小可爱回复“优惠”，还可领取直播限定折上折优惠哦～课纲&介绍&试学，详情请戳👉https://appze9inzwc2314.pc.xiaoe-tech.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30dbe0-6fcc-4f2e-a6e6-5385a8492a41",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h2yjdm3r92j21qe0f8gsr.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74554cd-340d-4739-9975-04b11382fca2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d79ee-d707-4a09-aada-8dacd4045244",
   "metadata": {},
   "source": [
    "# <center> 《模型融合与深度森林专题公开课》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7ec1b-c609-4b4a-86ba-fd058bb42db5",
   "metadata": {},
   "source": [
    "# <center>Day 3.深度森林原理与实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52276d-554b-47bd-baf9-cf0207ae6db0",
   "metadata": {},
   "source": [
    "- 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11705b10-58fc-4ed4-b62c-572ad315ae48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 其他辅助模块\n",
    "import inspect, re\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# hyperopt优化器\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf76c2-7165-45d1-ac8b-a2c4c7dc9d67",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们正式进入到深度森林相关内容。在本节内容中，我们将重点探讨机器学习最新研究成果——深度森林算法的基本原理及其实践技巧。当然，正如公开课开篇所言，我们学习深度森林，不只是掌握的算法增加一项，更关键的是，深度森林的本质其实是一种基于随机森林的模型融合策略，通过学习深度森林，我们从中窥见一些模型融合的核心思想和精彩策略，对于提升我们的模型融合技巧也有着至关重要的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab71565-3536-49f6-81b4-3eb63a751345",
   "metadata": {},
   "source": [
    "## 一、深度森林算法原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14251ce-5a51-4847-912e-b9c94715f833",
   "metadata": {},
   "source": [
    "- 方法提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b00282-c2f3-42e6-953f-3858f89ca549",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不可否认，深度森林的提出很大程度是受深度学习算法启发。现如今，深度学习算法在诸多领域都展示出了傲人的实力，周志华教授作为国内集成学习领域的先驱，则在借鉴了深度学习算法结构的基础上，提出了深度森林算法。我们可以说深度森林是深度学习算法的一种变种，但按照周教授的说法，更准确的来说，深度森林应该是集成学习的一个重大突破。深度森林论文已共享在公开课的网盘链接中，接下来我们对照论文进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34676ccd-bfdd-4d36-a2fd-9ed53984bc52",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据周志华教授的观点，深度神经网络的成功主要归结为三点，分别是逐层处理（layer-by-layer processing）、模型内特征变化（in-model feature transformation）以及模型复杂度（sufficient model complexity）。而在此基础之上，如果将神经网络中可微模块替换成不可微模块，例如随机森林，则能够在确保模型有效性和稳定性的基础上，减少模型对于反向传播的依赖。并且经过实验证明，采用类似神经网络的结果进行随机森林的集成，能够获得一个超参数效果稳定、算法复杂性可控的全新的模型——这个模型就是深度森林。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0bde9-21be-47f2-9a1f-65c6f7c949d4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;深度森林又名多粒度扫描的级联森林，这也是深度森林中最核心的两个概念，其一是级联森林，其二则是多粒度扫描。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741b48a-d2e4-4622-9ba0-c85c521da5a2",
   "metadata": {},
   "source": [
    "### 1.基本结构：级联森林（cascade forest）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108fb71-329a-401f-b0d2-411880b1404b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓的级联森林，指的构建一种类似神经网络的处理结构，如下所示，其中级联的每一级接收其前一级处理的特征信息，并将其处理结果输出到下一级。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13528f-0376-4e36-8b7f-6b51dec4146e",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h30py4j87mj219o0ka0w3.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bc067-49a9-4c3c-b9d0-c9308c98aaf3",
   "metadata": {},
   "source": [
    "而和神经网络不同的是，这里构成网络状结构的节点不再是激活函数，而是随机森林。其中每一层的深度森林包括两种模型，完全随机森林（completely-random tree forests）（蓝色）和随机森林（random forests）（黑色），在实际使用过程中，每一层两类模型的种类是超参数，可以人工调节。上图为了见简化其间，展示了两个随机森林与两个极端随机树集成的情况。并且在默认情况下，每个模型都包含500个弱分类器，即完全随机森林包含500个完全随机决策树，而随机森林模型则包含500个决策树模型。并且在默认情况下，每个模型只接收$\\sqrt{d}$个特征，其中d是总特征数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa1484-7c82-41ba-ac23-af33d35f22a1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述结构实际上就是一个集成算法的集成（ensemble of ensembles），而$\\sqrt{d}$个特征的分配，实际上也是继承自随机森林的参数设置经验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0c174-53fa-44e3-ba44-c0742f6683d0",
   "metadata": {},
   "source": [
    "### 2.单模型训练与强化特征矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d2dec-39fa-452f-a9da-a585d8dc92a3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，非常关键的环节是，在实际训练过程，每个随机森林模型（也包括完全随机森林）的输出并不是模型的预测概率，而是叶节点中不同样本的数量占比，并且是按照每一条数据来进行的计算。例如当我们带入全部数据集进行一个随机森林进行计算时，假设该随机森林只有3颗树（实际默认情况是500颗树，上述情况只是简化情况），那么在每一棵树中，某条条数据x（也就是某个样本）一定被分在了树的某个叶节点上，例如下图所示的第一棵树的最左边叶节点、第二棵树的中间叶节点、第三棵树的右边叶节点，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c645f2-ef7e-4ca5-b6c0-cb80bf1f5e6f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h30tcx31hyj213i0ewgnq.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cfcaa-6660-4a19-8d75-ec4c4569c98b",
   "metadata": {},
   "source": [
    "此时样本总共有三个不同类别，分别是绿色圆圈、红色的方框和黄色的三角，第一棵树上叶节点上三个不同类别样本占比分别为0.2：0.5：0.3，第二棵树上叶节点中三类占比为0.3：0.4：0.3，第三棵上叶节点中三类占比为1：0：0，然后计算三个类别占比的均值，即："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b69c538-b20f-4534-98b5-a1434b9724af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.3, 0.2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0.2, 0.5, 0.3]) + np.array([0.3, 0.4, 0.3]) + np.array([1, 0, 0])) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8dfbf-ece8-4f45-bbea-024a117e95e9",
   "metadata": {},
   "source": [
    "由此x样本就可以获得额外的3个特征（也被称作增强特征），即0.5、0.3、0.2。需要注意的是，每个样本都可以按照这个规则进行计算，假设当前数据集有m条数据，那么把这m条数据各自的增强特征按照样本顺序进行拼接，则可以拼接成规模为m行3列的增强特征集。即按照上述规则，一个随机森林即可创造一个包含3个特征的数据集，而如果进一步增加模型数量，即带入两个随机森林和两个完全随机森林进行计算，则可以创造包含3\\*4=12个特征的增强特征数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31487e7-3edb-490c-bda4-d6e7e844d2b4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里需要简单讨论，为何要求每个评估器输出叶节点上不同类别样本占比而不是模型预测结果，这里其实有两层考虑：首先，对于一个随机森林集成算法的集成，其本质仍然还是借助决策树的分类原理进行预测，即希望通过某些规则的叠加最终提升叶节点的不同样本类别的纯度；其次由于当前算法是借鉴了神经网络算法结构，而神经网络算法的核心优势就在于不同层其实能进行“有侧重”的学习，而其实深度森林也希望最终模型的不同层能呈现出“有侧重”的学习情况。当然，这也是后续模型为何要多粒度扫描的核心原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923cf16-e0af-457e-9e30-3913f992eb36",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，与其说模型输出的是样本预测结果，不如说此时每个单独的随机森林输出的是某种形式的模型分类效力指标。无论如何，每个模型的输出结果将成为一个增强特征（augmented features），当该层有4个模型时候，下一层将接收到12个增强特征。不难看出，这种增强特征数量较少，而在深度森林论文中，作者给出了这种增强特征“少但有效”的证明，我们只需要知道其确实有效即可。此外，为了减少过拟合风险，一般来说深度森林的每个随机森林在进行计算时也是需要经过交叉验证的，即得到的样本占比的均值，实际上是经过交叉验证后得到的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b6e52-b49f-497c-b845-2ba328c3c85a",
   "metadata": {},
   "source": [
    "### 3.滑动窗口与多粒度扫描（Multi-Grained Scanning）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912e8a8-e8f3-4039-8d0b-483363dd67d7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解了每个模型的训练过程和输出结果之后，接下来进一步看不同层的训练过程。这里最重要的概念就是多粒度扫描（Multi-Grained Scanning）。所谓的多粒度扫描，实际上指的是每一层接收到的特征和样本各不相同，从数据集角度来看，这个过程就类似于随着模型训练，逐渐扫描二维数据集。很明显，这个过程也是受到卷机神经网络和循环神经网络的启发——通过滑动窗口来学习完整数据集的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5edd1c-25d1-46d4-925e-98be92ce7d92",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h30ueuv7caj219i0rqgqj.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de24f01-8aa3-49b2-bff0-e58371beab4e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;先举个简单的例子了解什么是滑动窗口，假设现在某数据集有5个特征，我们以两个特征为窗口大小，滑动窗口的过程总共能创造4个包含两个特征的数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380832d-15d7-4cf8-96b9-1bb41ea2eced",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h30vpc43z8j21460beab3.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da910a7d-312a-405a-aa78-cff4f58e6a9c",
   "metadata": {},
   "source": [
    "类似的，如果现在数据集有100个特征，并且以10为窗口大小，滑动过程中就能创建301个包含100个特征的数据集。这其实就相当于是一种划分数据集的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860fccb-a1d4-4750-88bf-0128aacf4c19",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，针对这301个包含100个特征的数据集，我们可以按照之前所说的单模型训练过程，逐个带入这些数据集进行训练。注意，虽然是训练，但实际这也是一个数据增强的过程，即每个数据集训练完毕之后都将创造一个m\\*3的强化特征矩阵（假设总共是m条样本），而总共有301个数据集，因此一个模型就将产生一个包含301\\*3=903个特征的强化特征矩阵。而如果是带入两个模型，则将创造1806个特征的强化特征矩阵，即创造一个m\\*1806的数据集。注意该数据集也被称为原始强化数据集，需要带入到后续模型中进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9094704-8ce6-4094-b3e7-00d639022333",
   "metadata": {},
   "source": [
    "### 4.深度森林的模型训练和预测过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3bc71-ddd2-4cf6-adb4-39b023bcff8d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解了强化特征矩阵的创建过程后，接下来我们进一步来讨论如何进行级联训练。整个训练过程可以由下图进行表示，我们逐部分进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e703c8e-087e-4673-96c1-20d91b9ff59d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/e6c9d24ely1h30vyaxuamj21wm0rw101.jpg\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7e72b-d1d7-45c0-9a40-303c688ebf5e",
   "metadata": {},
   "source": [
    "- 通过多粒度扫描创建原始增强特征矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8070c78-265b-4e8c-8cf4-b36a32e16e30",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是通过多粒度扫描创建原始增强特征矩阵。需要注意的是，之前我们介绍过，在原始数据集有400个特征，并且窗口大小在100的情况下，通过一个随机森林和一个完全随机森林两个模型，就能创造一个m\\*1806的增强特征矩阵。而这个滑动窗口的大小是可以调整的，例如如果将窗口大小设置为200，则能创造201个包含200个特征的的数据集，按照每个数据集带入一个模型能创建3个增强特征的情况来进行计算，201个数据集\\*3\\*2个模型=能创建1206个特征的增强数据集。类似的如果窗口为300，则总共能创建101个数据集，带入两个模型则最终能创建一个包含606个特征的增强数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a80cf3-daeb-489a-90b1-9cd3b55fef2f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来看，在三个不同窗口大小设置的情况下，最终我们能获得A、B、C三个增强特征数据集。接下来我们就用这三个数据集来进行级联模型的训练。当然需要注意的是，这里选择几个窗口、每个窗口选择多少特征，都是可以作为超参数进行人工调整的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814d220-50df-451c-b46b-6305935b6337",
   "metadata": {},
   "source": [
    "- 级联模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82cc2e2-a244-43b6-a304-9c73455269b7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在准备了基础数据集之后，接下来开始进行级联模型训练。如上图所示，每一层我们都打算训练一组2个随机森林+2个完全随机森林的模型组，这里我们首先带入数据集A完成这第一组组模型的训练，训练完成后该组模型同样也将创建一个m\\*12的增强特征数据集。我们将该数据集与A数据集拼接，就得到了一个m\\*1818增强数据集，然后以该数据集训练第二层模型组，注意，每一层的模型组都是相同的，第二层模型组仍然是2个随机森林+2个完全随机森林，并且第二层模型组的训练和第一层模型组训练过程完全相同，在训练结束后将创造一个m\\*12增强特征矩阵，我们将其和数据集B进行拼接，最终构成一个m\\*1218数据集，并带入到第三层进行计算。第三层计算过程也类似，结束后得到一个C矩阵拼接后的增强特征矩阵（$Level 1_C$），并且得到一组预测结果。此时我们可以根据实际情况是否构建第二层的级联模型，如果继续构建下一层级联模型，则以$Level 1_C$作为初始矩阵进行带入训练，再次进行训练，如此往复。整体来看，这个过程其实就是级联的级联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f716c-d716-4ee4-930c-3cfc6acd5982",
   "metadata": {},
   "source": [
    "- 收敛条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ab41c-f0a5-47b9-b711-f5ca074cf420",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那么何时训练停止呢？一方面我们可以根据实际硬件情况决定迭代停止条件，此外也可以比较两个相邻的级联结果差异，如果发现增加一个级联并不能大幅提升模型效果，则可以考虑停止增加级联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b935a-4b99-44a6-91e6-5005e817a810",
   "metadata": {},
   "source": [
    "- 预测过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef8fc5-1f5a-4f51-bc58-121b8ace76d2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而当我们完成了模型训练之后，接下来就可以进行预测了。实际上深度森林的预测过程会非常高效，当带入某条测试数据时，我们只需要根据最终预测预测结果，即先寻找该样本的最高评分（即某个随机森林模型的叶节点的最高平均占比），然后根据该叶节点的类别判定当前样本所属类别即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f515c-5369-4e79-b956-de401e427872",
   "metadata": {},
   "source": [
    "## 二、深度森林算法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffc67e-e1ba-41da-a591-34f35354d733",
   "metadata": {},
   "source": [
    "### 1.深度森林安装与使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0816d-f138-47d9-b092-8e0ecedbfbf0",
   "metadata": {},
   "source": [
    "- 深度森林算法库安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45305c86-9791-4cdf-a536-75341db0ba19",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们可以使用pip快速安装深度森林算法库："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817c425-1518-4c99-8536-6acbe8b859eb",
   "metadata": {},
   "source": [
    "<center> pip install deep-forest -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c3d3f-43ec-4ab4-b39d-3b13a6ec76e2",
   "metadata": {},
   "source": [
    "然后即可按照如下方式进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f874127-88df-40a2-85d2-060c5e58080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepforest import CascadeForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efcfdd63-03e1-439a-b772-0677c909806c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mCascadeForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbin_subsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbin_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'percentile'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_trees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_predictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'forest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpredictor_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'custom'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_tolerant_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpartial_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Implementation of the deep forest for classification.\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_bins : :obj:`int`, default=255\n",
       "    The number of bins used for non-missing values. In addition to the\n",
       "    ``n_bins`` bins, one more bin is reserved for missing values. Its\n",
       "    value must be no smaller than 2 and no greater than 255.\n",
       "bin_subsample : :obj:`int`, default=200,000\n",
       "    The number of samples used to construct feature discrete bins. If\n",
       "    the size of training set is smaller than ``bin_subsample``, then all\n",
       "    training samples will be used.\n",
       "max_layers : :obj:`int`, default=20\n",
       "    The maximum number of cascade layers in the deep forest. Notice that\n",
       "    the actual number of layers can be smaller than ``max_layers`` because\n",
       "    of the internal early stopping stage.\n",
       "criterion : :obj:`{\"gini\", \"entropy\"}`, default= :obj:`\"gini\"`\n",
       "    The function to measure the quality of a split. Supported criteria \n",
       "    are ``gini`` for the Gini impurity and ``entropy`` for the information \n",
       "    gain. Note: this parameter is tree-specific.\n",
       "n_estimators : :obj:`int`, default=2\n",
       "    The number of estimator in each cascade layer. It will be multiplied\n",
       "    by 2 internally because each estimator contains a\n",
       "    :class:`RandomForestClassifier` and a :class:`ExtraTreesClassifier`,\n",
       "    respectively.\n",
       "n_trees : :obj:`int`, default=100\n",
       "    The number of trees in each estimator.\n",
       "max_depth : :obj:`int`, default=None\n",
       "    The maximum depth of each tree. ``None`` indicates no constraint.\n",
       "min_samples_leaf : :obj:`int`, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "use_predictor : :obj:`bool`, default=False\n",
       "    Whether to build the predictor concatenated to the deep forest. Using\n",
       "    the predictor may improve the performance of deep forest.\n",
       "predictor : :obj:`{\"forest\", \"xgboost\", \"lightgbm\"}`, default= :obj:`\"forest\"`\n",
       "    The type of the predictor concatenated to the deep forest. If\n",
       "    ``use_predictor`` is False, this parameter will have no effect.\n",
       "predictor_kwargs : :obj:`dict`, default={}\n",
       "    The configuration of the predictor concatenated to the deep forest.\n",
       "    Specifying this will extend/overwrite the original parameters inherit\n",
       "    from deep forest. If ``use_predictor`` is False, this parameter will\n",
       "    have no effect.\n",
       "backend : :obj:`{\"custom\", \"sklearn\"}`, default= :obj:`\"custom\"`\n",
       "    The backend of the forest estimator. Supported backends are ``custom``\n",
       "    for higher time and memory efficiency and ``sklearn`` for additional\n",
       "    functionality.\n",
       "n_tolerant_rounds : :obj:`int`, default=2\n",
       "    Specify when to conduct early stopping. The training process\n",
       "    terminates when the validation performance on the training set does\n",
       "    not improve compared against the best validation performance achieved\n",
       "    so far for ``n_tolerant_rounds`` rounds.\n",
       "delta : :obj:`float`, default=1e-5\n",
       "    Specify the threshold on early stopping. The counting on\n",
       "    ``n_tolerant_rounds`` is triggered if the performance of a fitted\n",
       "    cascade layer does not improve by ``delta`` compared against the best\n",
       "    validation performance achieved so far.\n",
       "partial_mode : :obj:`bool`, default=False\n",
       "    Whether to train the deep forest in partial mode. For large\n",
       "    datasets, it is recommended to use the partial mode.\n",
       "\n",
       "    - If ``True``, the partial mode is activated and all fitted\n",
       "      estimators will be dumped in a local buffer;\n",
       "    - If ``False``, all fitted estimators are directly stored in the\n",
       "      memory.\n",
       "n_jobs : :obj:`int` or ``None``, default=None\n",
       "    The number of jobs to run in parallel for both :meth:`fit` and\n",
       "    :meth:`predict`. None means 1 unless in a\n",
       "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
       "    processors.\n",
       "random_state : :obj:`int` or ``None``, default=None\n",
       "\n",
       "    - If :obj:`int`, ``random_state`` is the seed used by the random\n",
       "      number generator;\n",
       "    - If ``None``, the random number generator is the RandomState\n",
       "      instance used by :mod:`np.random`.\n",
       "verbose : :obj:`int`, default=1\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "    - If ``<= 0``, silent mode, which means no logging information will\n",
       "      be displayed;\n",
       "    - If ``1``, logging information on the cascade layer level will be\n",
       "      displayed;\n",
       "    - If ``> 1``, full logging information will be displayed.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages\\deepforest\\cascade.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CascadeForestClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c624903-35c6-41ec-8b48-5c150932eef0",
   "metadata": {},
   "source": [
    "当然，如果是回归问题，则可以导入回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "689c6a64-1fd4-47b0-b0c4-2fe9c149dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepforest import CascadeForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92170a8b-61c0-435b-89a6-3b64f2c94c92",
   "metadata": {},
   "source": [
    "- 深度森林算法初步实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fc8ca-0bda-4930-b1f1-96740e65c675",
   "metadata": {},
   "source": [
    "&emsp;&emsp;深度森林整体风格和sklearn非常一致，我们可以尝试导入数据集进行建模预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac042b7d-663c-4f09-b727-dce4fcbad7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-08 09:28:13.719] Start to fit the model:\n",
      "[2022-06-08 09:28:13.719] Fitting cascade layer = 0 \n",
      "[2022-06-08 09:28:14.169] layer = 0  | Val Acc = 97.921 % | Elapsed = 0.450 s\n",
      "[2022-06-08 09:28:14.173] Fitting cascade layer = 1 \n",
      "[2022-06-08 09:28:14.629] layer = 1  | Val Acc = 98.144 % | Elapsed = 0.456 s\n",
      "[2022-06-08 09:28:14.631] Fitting cascade layer = 2 \n",
      "[2022-06-08 09:28:15.029] layer = 2  | Val Acc = 97.699 % | Elapsed = 0.397 s\n",
      "[2022-06-08 09:28:15.029] Early stopping counter: 1 out of 2\n",
      "[2022-06-08 09:28:15.031] Fitting cascade layer = 3 \n",
      "[2022-06-08 09:28:15.418] layer = 3  | Val Acc = 96.956 % | Elapsed = 0.387 s\n",
      "[2022-06-08 09:28:15.418] Early stopping counter: 2 out of 2\n",
      "[2022-06-08 09:28:15.418] Handling early stopping\n",
      "[2022-06-08 09:28:15.418] The optimal number of layers: 2\n",
      "[2022-06-08 09:28:15.419] Start to evalute the model:\n",
      "[2022-06-08 09:28:15.419] Evaluating cascade layer = 0 \n",
      "[2022-06-08 09:28:15.440] Evaluating cascade layer = 1 \n",
      "\n",
      "Testing Accuracy: 98.444 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=1)\n",
    "model = CascadeForestClassifier(random_state=8)\n",
    "model.fit(X_train1, y_train1)\n",
    "y_pred = model.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb6acb-2314-4658-8743-90e5defc3ad8",
   "metadata": {},
   "source": [
    "尽管深度森林原理复杂，但实际的使用过程秉承了sklearn的一贯风格——代码简单，并且拥有一整套效果较好的初始化参数。接下来简单对比决策树和随机森林模型效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5cac8f37-30f7-42de-84d6-a6d8551dd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 84.889 %\n"
     ]
    }
   ],
   "source": [
    "# 决策树模型训练过程\n",
    "model = DecisionTreeClassifier(random_state=8)\n",
    "model.fit(X_train1, y_train1)\n",
    "y_pred = model.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f3fa3167-680b-43f8-a1ce-3ae8fa1b8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 98.222 %\n"
     ]
    }
   ],
   "source": [
    "# 随机森林模型训练过程\n",
    "model = RandomForestClassifier(random_state=8)\n",
    "model.fit(X_train1, y_train1)\n",
    "y_pred = model.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2230bd65-bca2-4296-9eea-c6b169411297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41003e57-bdc3-4bf3-b0bd-c806d419ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 97.111 %\n"
     ]
    }
   ],
   "source": [
    "# XGBoost模型预测结果\n",
    "model = XGBClassifier(random_state=8)\n",
    "model.fit(X_train1, y_train1)\n",
    "y_pred = model.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69ec22ba-043a-44ed-8b01-7917a4ee12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 96.444 %\n"
     ]
    }
   ],
   "source": [
    "# LGBM模型预测结果\n",
    "model = LGBMClassifier(random_state=8)\n",
    "model.fit(X_train1, y_train1, categorical_feature = 'auto',eval_set=(X_test1,y_test1),feature_name='auto', verbose=0)\n",
    "y_pred = model.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2433c5-45e2-49bc-92be-f1ab94ac70c5",
   "metadata": {},
   "source": [
    "能够看出深度森林实际效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b946caa-2567-43f4-997f-70b674083861",
   "metadata": {},
   "source": [
    "|Models|test_score|\n",
    "|:--:|:--:|\n",
    "|CascadeForest|98.444 %|\n",
    "|DecisionTree|84.889 %|\n",
    "|RandomForest|98.222 %|\n",
    "|XGBoost|97.111 %|\n",
    "|LGBM|96.444 %|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ebad4-2057-4e97-a09c-448f69c292f3",
   "metadata": {},
   "source": [
    "- 深度森林算法抗过拟合特性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab779a-b90c-44e8-a7a6-6b72d8070096",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，深度森林其实拥有天然的抗过拟合特性（自带交叉验证过程）。因此在不进行任何参数调整情况下，在当前案例数据集上也能得到一个较好的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85db2d46-233d-4dec-93e9-46d3b3a38541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-08 09:26:52.932] Start to fit the model:\n",
      "[2022-06-08 09:26:52.932] Fitting cascade layer = 0 \n",
      "[2022-06-08 09:26:53.785] layer = 0  | Val Acc = 79.137 % | Elapsed = 0.853 s\n",
      "[2022-06-08 09:26:53.790] Fitting cascade layer = 1 \n",
      "[2022-06-08 09:26:54.929] layer = 1  | Val Acc = 80.178 % | Elapsed = 1.139 s\n",
      "[2022-06-08 09:26:54.934] Fitting cascade layer = 2 \n",
      "[2022-06-08 09:26:56.085] layer = 2  | Val Acc = 80.121 % | Elapsed = 1.151 s\n",
      "[2022-06-08 09:26:56.085] Early stopping counter: 1 out of 2\n",
      "[2022-06-08 09:26:56.090] Fitting cascade layer = 3 \n",
      "[2022-06-08 09:26:57.245] layer = 3  | Val Acc = 80.329 % | Elapsed = 1.155 s\n",
      "[2022-06-08 09:26:57.250] Fitting cascade layer = 4 \n",
      "[2022-06-08 09:26:58.403] layer = 4  | Val Acc = 79.799 % | Elapsed = 1.153 s\n",
      "[2022-06-08 09:26:58.403] Early stopping counter: 1 out of 2\n",
      "[2022-06-08 09:26:58.408] Fitting cascade layer = 5 \n",
      "[2022-06-08 09:26:59.557] layer = 5  | Val Acc = 80.140 % | Elapsed = 1.149 s\n",
      "[2022-06-08 09:26:59.557] Early stopping counter: 2 out of 2\n",
      "[2022-06-08 09:26:59.557] Handling early stopping\n",
      "[2022-06-08 09:26:59.558] The optimal number of layers: 4\n",
      "[2022-06-08 09:26:59.560] Start to evalute the model:\n",
      "[2022-06-08 09:26:59.561] Evaluating cascade layer = 0 \n",
      "[2022-06-08 09:26:59.621] Evaluating cascade layer = 1 \n",
      "[2022-06-08 09:26:59.683] Evaluating cascade layer = 2 \n",
      "[2022-06-08 09:26:59.744] Evaluating cascade layer = 3 \n",
      "\n",
      "Testing Accuracy: 79.046 %\n"
     ]
    }
   ],
   "source": [
    "model = CascadeForestClassifier(random_state=8)\n",
    "model.fit(X_train_OE.values, y_train.values)\n",
    "y_pred = model.predict(X_test_OE)\n",
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f47fe-b658-4be2-b144-7d4f43e24b19",
   "metadata": {},
   "source": [
    "能够发现，基本能够获得一个和随机森林调参后类似的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6894d49-c28f-4d58-8aed-24db91051e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 77.740 %\n"
     ]
    }
   ],
   "source": [
    "# XGBoost模型预测结果\n",
    "model = XGBClassifier(random_state=8)\n",
    "model.fit(X_train_OE, y_train)\n",
    "y_pred = model.predict(X_test_OE)\n",
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "08394d75-699f-48df-8935-7133c7a59e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 78.705 %\n"
     ]
    }
   ],
   "source": [
    "# LGBM模型预测结果\n",
    "model = LGBMClassifier(random_state=8)\n",
    "model.fit(X_train_OE, y_train, categorical_feature = 'auto',eval_set=(X_test_OE,y_test),feature_name='auto', verbose=0)\n",
    "y_pred = model.predict(X_test_OE)\n",
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2c1ce4d-cb47-4fad-b6b3-63c002e91474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 79.273 %\n"
     ]
    }
   ],
   "source": [
    "# CatBoost模型预测结果\n",
    "categorical_features_indices = np.where(X_train_OE.dtypes != np.float)[0]\n",
    "\n",
    "model = CatBoostClassifier(verbose=False,random_state=8)\n",
    "model.fit(X_train_OE, y_train,cat_features=categorical_features_indices,eval_set=(X_test_OE, y_test))\n",
    "y_pred = model.predict(X_test_OE)\n",
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"\\nTesting Accuracy: {:.3f} %\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef1407-7431-4ad0-8a17-424a64de9876",
   "metadata": {},
   "source": [
    "|Models|test_score|\n",
    "|:--:|:--:|\n",
    "|CascadeForest|79.046 %|\n",
    "|XGBoost|77.740 %|\n",
    "|LGBM|78.705%|\n",
    "|CatBoost|79.273 %|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
